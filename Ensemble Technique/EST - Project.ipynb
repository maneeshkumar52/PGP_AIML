{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Â©Great Learning. Proprietary content. All Rights Reserved. Unauthorised use or distribution prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIML Module Project - ENSEMBLE TECHNIQUES - Project \n",
    "\n",
    "- Learner Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> Assessment [ Total Score: 60 points ]\n",
    "\n",
    "Please refer to the problem statement for questions and descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and warehouse data: \n",
    "\n",
    "- Import all the given datasets from MYSQL server. Explore shape and size. \n",
    "- Merge all datasets onto one and explore final shape and size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Use MySQL server to fetch the data onto python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Libraries\n",
    "\n",
    "# pip install mysql-connector-python-rf\n",
    "# pip install PyMySQL\n",
    "\n",
    "# Open terminal in mac and copy \"conda install -c anaconda mysql-connector-python\"\n",
    "import mysql.connector\n",
    "\n",
    "# importing 'mysql.connector' as mysql for convenient\n",
    "import mysql.connector as mysql\n",
    "\n",
    "# open terminal in mac and put \"pip install PyMySQL\"\n",
    "import pymysql.cursors\n",
    "import pymysql\n",
    "\n",
    "\n",
    "\n",
    "# Connection setup\n",
    "MYSQL_Connection = pymysql.connect(host='',\n",
    "                             user='',\n",
    "                             password='',\n",
    "                             db=''\n",
    "                             )\n",
    "\n",
    "print(MYSQL_Connection)\n",
    "\n",
    "# Create a Cursor object to execute queries.\n",
    "cur1 = MYSQL_Connection.cursor()\n",
    "cur2 = MYSQL_Connection.cursor()\n",
    " \n",
    "# Select data from table using SQL query.\n",
    "cur1.execute(\"SELECT * FROM telcomcustomerchurn1\")\n",
    "cur2.execute(\"SELECT * FROM telcomcustomerchurn2\")\n",
    "\n",
    "# Import data from MY SQL server\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DB_1 = pd.read_sql('SELECT * FROM telcomcustomerchurn1', con=MYSQL_Connection)\n",
    "DB_2 = pd.read_sql('SELECT * FROM telcomcustomerchurn2', con=MYSQL_Connection)\n",
    "\n",
    "DB_1.head()\n",
    "DB_2.head()\n",
    "\n",
    "print(\"Data 1:\",DB_1.shape)\n",
    "print(\"\\nData 2:\",DB_2.shape)\n",
    "\n",
    "\n",
    "DB_1.to_excel('DB1.xlsx')\n",
    "DB_2.to_excel('DB2.xlsx')\n",
    "\n",
    "DB_Original = pd.concat([DB_1, DB_2], axis=1, sort=False) \n",
    "DB=DB_Original.copy(deep=True) # Backup the original data\n",
    "DB.head()\n",
    "print(\"\\nConcatenated Data :\",DB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "#### Using python to query\n",
    "\n",
    "https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-transaction.html\n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Use pandas to import the data from your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DB_1 = pd.read_csv('TelcomCustomer-Churn_1.csv')\n",
    "DB_2 = pd.read_csv('TelcomCustomer-Churn_2.csv')\n",
    "print(\"Data 1:\",DB_1.shape)\n",
    "print(\"\\nData 2:\",DB_2.shape)\n",
    "DB_Original = pd.concat([DB_1, DB_2], axis=1, sort=False) \n",
    "DB=DB_Original.copy(deep=True) # Backup the original data\n",
    "DB.head()\n",
    "print(\"\\nConcatenated Data :\",DB.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data cleansing: \n",
    "\n",
    "- Missing value treatment\n",
    "- Convert categorical attributes to continuous using relevant functional knowledge\n",
    "- Drop attribute/s if required using relevant functional knowledge\n",
    "- Automate all the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data types \n",
    "DB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate categorical vs numerical attributes separation\n",
    "Summary=DB.describe().T\n",
    "Conti=Summary.index\n",
    "Column_name=DB.columns\n",
    "All=[]\n",
    "for i in range(len(Conti)):\n",
    "    for j in range(len(Column_name)):\n",
    "        if Conti[i]!=Column_name[j]:\n",
    "            All.append(Column_name[j])\n",
    "Categorical=list(set(All)-set(Conti))\n",
    "Continuous=list(set(All)-set(Categorical))\n",
    "print(\"Continuous Variables:\",Continuous)\n",
    "print(\"\\nCategorical variables found in the dataset:\\n\",Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value treatment\n",
    "\n",
    "# Styling\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "Row=DB.shape[0]\n",
    "Column=DB.shape[1]\n",
    "\n",
    "print(Fore.GREEN+\"\\nMissing or null values:\")\n",
    "print(Style.RESET_ALL)\n",
    "print(DB.isnull().sum())\n",
    "\n",
    "\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "print(Fore.GREEN+\"\\n\\nDo you wish to do any replacements ? \",Fore.BLACK+\"<yes> or <no>\")\n",
    "answer_replacement=str(input())\n",
    "\n",
    "if answer_replacement=='yes':\n",
    "    print(Fore.GREEN+\"\\nDo you want to replce missing values by\",Fore.BLACK+\"<zero> or <median> or <mean> or <drop> ?\")\n",
    "    print(Style.RESET_ALL)\n",
    "    Replace_option=str(input())\n",
    "    if Replace_option=='zero':\n",
    "        DB = DB.fillna('0')\n",
    "        print(Fore.GREEN+\"Replaced all missing values by ZERO\")\n",
    "    elif Replace_option=='median':\n",
    "        DB = DB.replace({'NaN':DB.median()})\n",
    "        print(Fore.GREEN+\"Replaced all missing values by MEDIAN\")\n",
    "    elif Replace_option=='mean':\n",
    "        DB = DB.replace({'NaN':DB.mean()})\n",
    "        print(Fore.GREEN+\"Replaced all missing values by MEAN\")\n",
    "    elif Replace_option=='drop':\n",
    "        DB = DB.dropna()\n",
    "        print(Fore.GREEN+\"Rows with mising values have been dropped\")\n",
    "        print(Fore.GREEN+\"\\nCurrent shape of dataset is:\",Fore.BLACK+\"\",DB.shape)\n",
    "        # Variables \"ROW & Column\" defined during data input. Size of original data\n",
    "        new_size=100-(100*DB.size/(Row*Column))  \n",
    "        print(Fore.GREEN+\"\\nDataset has lost\",Fore.BLACK+\"\",new_size,\"% of data\")\n",
    "        if new_size>50:\n",
    "            print(Fore.RED+\"Caution: We have lost majority of our data and related info \")\n",
    "    print(Style.RESET_ALL) \n",
    "\n",
    "elif answer_replacement=='no':\n",
    "    print(\"Data set good to be used, nothing to replace \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical columns to continuous\n",
    "\n",
    "# Manipulation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print(Fore.GREEN+\"\\nCategorical variables found in the dataset:\\n\",Fore.BLACK+\"\",Categorical)\n",
    "print(Fore.GREEN+\"\\n\\nDo you wish to convert categorical features to continuous ? \",Fore.BLACK+\"<yes> or <no>\")\n",
    "answer_cat_con=str(input())\n",
    "le = preprocessing.LabelEncoder()\n",
    "if answer_cat_con=='yes':\n",
    "    print(Fore.GREEN+\"Enter the name of feature columns separated by <,>\")\n",
    "    Transform=str(input())\n",
    "    print(Fore.GREEN+\"\\nYou have chosen the following:\\n\",Fore.BLACK+\"\",Transform)\n",
    "    Trans = list(Transform.split(\",\"))\n",
    "    for i in range(len(Trans)):\n",
    "        a=Trans[i]\n",
    "        DB[a] = le.fit_transform(DB[a])\n",
    "\n",
    "print(Fore.GREEN+\"\\nTop 5 rows of the dataset below:\\n\")\n",
    "print(Fore.BLACK+\"\",DB.head())\n",
    "print(Style.RESET_ALL) \n",
    "\n",
    "shape=DB.shape\n",
    "Row=shape[0]\n",
    "Column=shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to convert to categorical\n",
    "\n",
    "StreamingTV,PhoneService,PaymentMethod,OnlineSecurity,TechSupport,DeviceProtection,Contract,PaperlessBilling, Churn,InternetService,gender,Partner,StreamingMovies,MultipleLines,OnlineBackup,Dependents,TotalCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "\n",
    "print(Fore.GREEN+\"\\n\\nDo you wish to drop any columns ? \",Fore.BLACK+\"<yes> or <no>\")\n",
    "answer_drop=str(input())\n",
    "\n",
    "if answer_drop=='yes':\n",
    "    print(Fore.GREEN+\"\\nEnter the column names seprated by\",Fore.BLACK+\"<,>\")\n",
    "    print(Style.RESET_ALL)\n",
    "    Drop_col=str(input())\n",
    "    Trans = list(Drop_col.split(\",\"))\n",
    "    for i in range(len(Trans)):\n",
    "        a=Trans[i]\n",
    "        DB = DB.drop(Drop_col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final dataset shape\",DB.shape)\n",
    "DB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data analysis & visualisation: \n",
    "\n",
    "- Perform detailed statistical analysis on the data.\n",
    "- Perform a detailed univariate, bivariate and multivariate analysis with appropriate detailed comments after each analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform detailed statistical analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a detailed univariate, bivariate and multivariate analysis with appropriate detailed comments after each analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data pre-processing: \n",
    "\n",
    "- Segregate predictors vs target attributes\n",
    "- Check for target balancing and fix it if found imbalanced.\n",
    "- Perform train-test split.\n",
    "- Check if the train and test data have similar statistical characteristics when compared with original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split: Independent vs target variable\n",
    "\n",
    "y = DB['Churn']                  # Target variable\n",
    "X = DB.drop(['Churn'],axis=1)    # Predictor variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the train and test data have similar statistical characteristics when compared with original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Data distribution (combined data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplots(figsize=(20,5))\n",
    "plt.plot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Train Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)  # shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)   # Scale train data\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)     # Scale test data\n",
    "\n",
    "DB_Original_scaled=StandardScaler().fit_transform(DB)      # Scale original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=pd.DataFrame(X_train_scaled)\n",
    "Train_DB=Train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,5))\n",
    "plt.plot(X_train_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = pd.DataFrame(X_test_scaled)\n",
    "Test_DB=Test.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,5))\n",
    "plt.plot(X_test_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the sampld population re statistically equal\n",
    "\n",
    "Orig_DB=pd.DataFrame(DB_Original_scaled)\n",
    "Original_DB=Orig_DB.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare means between train and test data\n",
    "\n",
    "n=1\n",
    "accumulator_Train=0\n",
    "accumulator_Test=0\n",
    "\n",
    "# Original vs train\n",
    "for i in range(19):\n",
    "        Diff=Train_DB.iloc[i,n]-Original_DB.iloc[i,n]\n",
    "        print(\"Original vs train for feature \",i,\"is:\",Diff)\n",
    "        accumulator_Train=accumulator_Train+Diff\n",
    "\n",
    "print(\"Mean difference: \", accumulator_Train.mean())\n",
    "\n",
    "print(\"\\n\\nPlease wait . . . . . Processing in progress . . . . . \")\n",
    "time.sleep(10)    \n",
    "print(\"\\n\\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\\n \")\n",
    "\n",
    "# Original vs train\n",
    "for i in range(19):\n",
    "        Diff=Test_DB.iloc[i,n]-Original_DB.iloc[i,n]\n",
    "        print(\"Original vs test for feature \",i,\"is:\",Diff)\n",
    "        accumulator_Test=accumulator_Test+Diff\n",
    "\n",
    "print(\"Mean difference: \", accumulator_Test.mean())\n",
    "\n",
    "print(\"\\n\\nPlease wait . . . . . Processing in progress . . . . . \")\n",
    "time.sleep(10)    \n",
    "print(\"\\n\\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\\n \")\n",
    "\n",
    "\n",
    "Diff=accumulator_Train-accumulator_Test\n",
    "print(\"Difference between train and test:\",Diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Median between train and test data\n",
    "\n",
    "n=5\n",
    "accumulator_Train=0\n",
    "accumulator_Test=0\n",
    "\n",
    "# Original vs train\n",
    "for i in range(19):\n",
    "        Diff=Train_DB.iloc[i,n]-Original_DB.iloc[i,n]\n",
    "        print(\"Original vs train for feature \",i,\"is:\",Diff)\n",
    "        accumulator_Train=accumulator_Train+Diff\n",
    "\n",
    "print(\"Mean difference: \", accumulator_Train.mean())\n",
    "\n",
    "print(\"\\n\\nPlease wait . . . . . Processing in progress . . . . . \")\n",
    "time.sleep(10)    \n",
    "print(\"\\n\\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\\n \")\n",
    "\n",
    "# Original vs train\n",
    "for i in range(19):\n",
    "        Diff=Test_DB.iloc[i,n]-Original_DB.iloc[i,n]\n",
    "        print(\"Original vs test for feature \",i,\"is:\",Diff)\n",
    "        accumulator_Test=accumulator_Test+Diff\n",
    "\n",
    "print(\"Mean difference: \", accumulator_Test.mean())\n",
    "\n",
    "print(\"\\n\\nPlease wait . . . . . Processing in progress . . . . . \")\n",
    "time.sleep(10)    \n",
    "print(\"\\n\\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\\n \")\n",
    "\n",
    "\n",
    "Diff=accumulator_Train-accumulator_Test\n",
    "print(\"Difference between train and test:\",Diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model training, testing and tuning: \n",
    "\n",
    "- Train and test all ensemble models taught in the learning module.\n",
    "        - Suggestion: Use standard ensembles available. Also you can design your own ensemble technique using weak classifiers.\n",
    "- Display the classification accuracies for train and test data.\n",
    "- Apply all the possible tuning techniques to train the best model for the given data. \n",
    "        - Suggestion: Use all possible hyper parameter combinations to extract the best accuracies. \n",
    "- Display and compare all the models designed with their train and test accuracies.\n",
    "- Select the final best trained model along with your detailed comments for selecting this model. \n",
    "- Pickle the selected model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "from sklearn. linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LogR = LogisticRegression()\n",
    "LogR.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred=LogR.predict(X_train_scaled)\n",
    "LogR_Accuracy_Train = accuracy_score(y_true=y_train, y_pred=y_pred)\n",
    "print(\"Train Accuracy : \", LogR_Accuracy_Train)\n",
    "\n",
    "y_pred = LogR.predict(X_test_scaled)\n",
    "LogR_Accuracy_Test = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Test Accuracy : \", LogR_Accuracy_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB - Find the best NBs kernels\n",
    "\n",
    "k=[BernoulliNB,GaussianNB]\n",
    "for i in range (len(k)):\n",
    "    NB_Classifier = k[i]()\n",
    "    NB_Classifier.fit(X_train_scaled, y_train)\n",
    "    print ('kernel is =',k[i], '\\tScore=',NB_Classifier.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB - Model\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = NB.predict(X_train_scaled)\n",
    "NB_Accuracy_Train=accuracy_score(y_train, y_pred, normalize = True)\n",
    "print(\"Train Accuracy: \", NB_Accuracy_Train)\n",
    "\n",
    "y_pred = NB.predict(X_test_scaled)\n",
    "NB_Accuracy_Test=accuracy_score(y_test, y_pred, normalize = True)\n",
    "print(\"Test Accuracy : \", NB_Accuracy_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - to find the best parameters\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "c=np.arange(0.1,1.1,0.1)  # Range of C values\n",
    "\n",
    "kernels=['linear','rbf','sigmoid','poly']  # Range of kernels\n",
    "\n",
    "# gamma \n",
    "\n",
    "# Loop to find the best parameters for C and Kernels\n",
    "\n",
    "for k in range (len(kernels)):\n",
    "    for i in range (len(c)):\n",
    "        SVM_Classifier = SVC(C=c[i],kernel=kernels[k])\n",
    "        SVM_Classifier.fit(X_train_scaled, y_train)\n",
    "        print ('C=',round(c[i],2),\"\\tKernel=\",kernels[k],'\\tScore=',SVM_Classifier.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - Model\n",
    "from sklearn.svm import SVC\n",
    "SVM = SVC(C=0.9,kernel='linear')\n",
    "SVM.fit(X_train_scaled, y_train)\n",
    "\n",
    "SVM_Accuracy=SVM.score(X_train_scaled, y_train)\n",
    "print(\"Train Accuracy : \", SVM_Accuracy)\n",
    "\n",
    "SVM_Accuracy=SVM.score(X_test_scaled, y_test)\n",
    "print(\"Test Accuracy : \", SVM_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN - choosing the K value\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList = list(range(2,20))  # k=arange(1,20,2)\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold accuracy scores\n",
    "ac_scores = []\n",
    "\n",
    "# perform accuracy metrics for values from 1,3,5....19\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    # predict the response\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    # evaluate accuracy\n",
    "    scores = accuracy_score(y_test, y_pred)\n",
    "    ac_scores.append(scores)\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in ac_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN - Model\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=19)\n",
    "KNN.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = KNN.predict(X_train_scaled)\n",
    "KNN_Accuracy_Train=accuracy_score(y_train, y_pred)\n",
    "print(\"Train Accuracy : \", KNN_Accuracy_Train)\n",
    "\n",
    "y_pred = KNN.predict(X_test_scaled)\n",
    "KNN_Accuracy_Test=accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy : \", KNN_Accuracy_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "# Library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix\n",
    "\n",
    "# Model\n",
    "model_DT=DecisionTreeClassifier(criterion='entropy',  # 1\n",
    "                                splitter='best',     # 3\n",
    "                                max_depth=5,       # 2\n",
    "                                min_samples_split=2,  \n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0.0,\n",
    "                                max_features=None,\n",
    "                                random_state=None,\n",
    "                                max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None,\n",
    "                                class_weight=None,\n",
    "                                presort='deprecated',\n",
    "                                ccp_alpha=0.0)\n",
    "\n",
    "model_DT.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Accuracy\n",
    "DT_Train=model_DT.score(X_train_scaled, y_train)\n",
    "DT_Test=model_DT.score(X_test_scaled, y_test)\n",
    "\n",
    "# Output\n",
    "print(\"Train Accuracy:\",DT_Train)\n",
    "print(\"Test Accuracy:\",DT_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "#### Decision Tree - Hyper parameters\n",
    "\n",
    "1. criterion :(default=\"gini\"). The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "    \n",
    "\n",
    "2. splitter : (default=\"best\") The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
    "\n",
    "\n",
    "3. max_depth : (default=None) The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "4. min_samples_split : (default=2) The minimum number of samples required to split an internal node:\n",
    "\n",
    "\n",
    "5. min_samples_leaf : (default=1) The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "\n",
    "\n",
    "6. min_weight_fraction_leaf : (default=0.) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "7. max_features : (default=None) The number of features to consider when looking for the best split:\n",
    "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "\n",
    "\n",
    "8. random_state :(default=None) Integer value to be used.\n",
    "\n",
    "9. max_leaf_nodes : (default=None) Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "10. min_impurity_decrease : (default=0.) A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "    - The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    - where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "      samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "      left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "    - ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "       if ``sample_weight`` is passed.\n",
    "\n",
    "\n",
    "\n",
    "11. min_impurity_split : (default=1e-7) Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "\n",
    "12. class_weight : (default=None) Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "  \n",
    "\n",
    "13. presort : (default='deprecated') This parameter is deprecated\n",
    "\n",
    "\n",
    "14. ccp_alpha : (default=0.0) Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. \n",
    "\n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "# Library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Model\n",
    "model_RF = RandomForestClassifier(n_estimators=100,        # CV\n",
    "                                    criterion='gini',\n",
    "                                    max_depth=4,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_weight_fraction_leaf=0.0,\n",
    "                                    max_features='sqrt',\n",
    "                                    max_leaf_nodes=None,\n",
    "                                    min_impurity_decrease=0.0,\n",
    "                                    min_impurity_split=None,\n",
    "                                    bootstrap=True,\n",
    "                                    oob_score=False,\n",
    "                                    n_jobs=None,\n",
    "                                    random_state=None,\n",
    "                                    verbose=0,\n",
    "                                    warm_start=False,\n",
    "                                    class_weight=None,\n",
    "                                    ccp_alpha=0.001,\n",
    "                                    max_samples=None,)\n",
    "model_RF.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Accuracy\n",
    "pred_RF = model_RF.predict(X_test_scaled)\n",
    "RF_Train = model_RF.score(X_train_scaled, y_train)\n",
    "RF_Test = accuracy_score(y_test, pred_RF)\n",
    "\n",
    "# Output\n",
    "print(\"Train Accuracy:\",RF_Train)\n",
    "print(\"Test Accuracy:\",RF_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "Hyper parameters - RANDOM FOREST\n",
    "\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if `bootstrap=True` (default).\n",
    "\n",
    "\n",
    "1. n_estimators : (default=100) The number of trees in the forest.\n",
    "\n",
    "\n",
    "2. criterion : (default=\"gini\") The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "   \n",
    "   \n",
    "3. max_depth : (default=None) The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "4. min_samples_split : (default=2) The minimum number of samples required to split an internal node:\n",
    "\n",
    "\n",
    "5. min_samples_leaf : (default=1) The minimum number of samples required to be at a leaf node. A split point at any     depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "\n",
    "6. min_weight_fraction_leaf : (default=0.) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "7. max_features : (default=\"auto\") The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None, then `max_features=n_features`.\n",
    "\n",
    "   \n",
    "8. max_leaf_nodes : (default=None) Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "9. min_impurity_decrease : (default=0.) A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "   - The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "   - where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "     samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "     left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "   - ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "      if ``sample_weight`` is passed.\n",
    "\n",
    "\n",
    "10. min_impurity_split : (default=1e-7) Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "\n",
    "11. bootstrap : (default=True) Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n",
    "\n",
    "12. oob_score :  (default=False) Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "13. n_jobs : (default=None) The number of jobs to run in parallel. \n",
    " \n",
    "\n",
    "14. random_state : (default=None) Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``).\n",
    "    \n",
    "\n",
    "15. verbose : (default=0) Controls the verbosity when fitting and predicting.\n",
    "\n",
    "16. warm_start : (default=False) When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "17. class_weight : (default=None) Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "\n",
    "18. ccp_alpha : (default=0.0) Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
    "\n",
    "\n",
    "19. max_samples : int or float, default=None. If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADA Boosting\n",
    "\n",
    "# Library\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model\n",
    "model = AdaBoostClassifier( base_estimator=None,    # SVC(), RandomForestClassifier(), GradientBoostingClassifier()\n",
    "                            n_estimators=200,\n",
    "                            learning_rate=1.0,\n",
    "                            algorithm='SAMME.R',     # try using SVC() as base estimator above\n",
    "                            random_state=None,)\n",
    "model_AB = model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Accuracy\n",
    "AB_Train = model_AB.score(X_train_scaled, y_train)\n",
    "pred_AB =model_AB.predict(X_test_scaled)\n",
    "acc_AB = accuracy_score(y_test, pred_AB)\n",
    "\n",
    "print(\"Train Accuracy:\",AB_Train)\n",
    "print(\"Test Accuracy:\",acc_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\n",
    "#### Hyper Parameters - ADA Boosting\n",
    "\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "\n",
    "1. base_estimator : (default=None) The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper ``classes_`` and ``n_classes_`` attributes. If ``None``, then the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\n",
    "\n",
    "2. n_estimators : (default=50) The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "3. learning_rate : (default=1.) Learning rate shrinks the contribution of each classifier by ``learning_rate``. There is a trade-off between ``learning_rate`` and ``n_estimators``.\n",
    "\n",
    "4. algorithm : (default='SAMME.R')\n",
    "    - If 'SAMME.R' then use the SAMME.R real boosting algorithm. ``base_estimator`` must support calculation of class       probabilities.\n",
    "    - If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "    - The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting         iterations.\n",
    "\n",
    "5. random_state : (default=None)\n",
    "    - If int, random_state is the seed used by the random number generator;\n",
    "    - If RandomState instance, random_state is the random number generator;\n",
    "    - If None, the random number generator is the RandomState instance used by `np.random`.\n",
    "    \n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT BOOSTING\n",
    "\n",
    "\n",
    "# Library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Model\n",
    "model = GradientBoostingClassifier(loss='deviance',\n",
    "                                    learning_rate=0.1,\n",
    "                                    n_estimators=500,\n",
    "                                    subsample=1.0,\n",
    "                                    criterion='friedman_mse',\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_weight_fraction_leaf=0.0,\n",
    "                                    max_depth=3,\n",
    "                                    min_impurity_decrease=0.0,\n",
    "                                    min_impurity_split=None,\n",
    "                                    init=None,\n",
    "                                    random_state=None,\n",
    "                                    max_features=None,\n",
    "                                    verbose=1,\n",
    "                                    max_leaf_nodes=None,\n",
    "                                    warm_start=False,\n",
    "                                    presort='deprecated',\n",
    "                                    validation_fraction=0.1,\n",
    "                                    n_iter_no_change=None,\n",
    "                                    tol=0.0001,\n",
    "                                    ccp_alpha=0.0)\n",
    "model_GB = model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Accuracy\n",
    "GB_Train = model_GB.score(X_train_scaled, y_train)\n",
    "pred_GB =model_GB.predict(X_test_scaled)\n",
    "acc_GB = accuracy_score(y_test, pred_GB)\n",
    "\n",
    "print(\"Training Accuracy:\",GB_Train)\n",
    "print(\"Testing Accuracy:\",acc_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "#### Hyperparameters - Gradient Boosting\n",
    "\n",
    "Gradient Boosting for classification. GB builds an additive model in forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n",
    "\n",
    "\n",
    "1. loss : (default='deviance') loss function to be optimized. 'deviance' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss 'exponential' gradient boosting recovers the AdaBoost algorithm.\n",
    "\n",
    "2. learning_rate : (default=0.1) learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "3. n_estimators : (default=100) The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "\n",
    "4. subsample : (default=1.0) The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias.\n",
    "\n",
    "5. criterion :(default=\"friedman_mse\") The function to measure the quality of a split. Supported criteria are \"friedman_mse\" for the mean squared error with improvement score by Friedman, \"mse\" for mean squared error, and \"mae\" for the mean absolute error. The default value of \"friedman_mse\" is generally the best as it can provide a better approximation in some cases.\n",
    "\n",
    "  \n",
    "6. min_samples_split : (default=2) The minimum number of samples required to split an internal node:\n",
    "\n",
    "\n",
    "7. min_samples_leaf : (default=1) The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "  \n",
    "\n",
    "8. min_weight_fraction_leaf : (default=0.) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "9. max_depth : (default=3) maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "10. min_impurity_decrease : (default=0.) A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "    - The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    - where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is       the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "    - ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "       if ``sample_weight`` is passed.\n",
    "\n",
    "\n",
    "\n",
    "11. min_impurity_split : (default=1e-7) Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "   \n",
    "12. init :  (default=None) An estimator object that is used to compute the initial predictions.\n",
    "   \n",
    "   \n",
    "13. random_state : (default=None)\n",
    "    - If int, random_state is the seed used by the random number generator;\n",
    "    - If RandomState instance, random_state is the random number generator;\n",
    "    - If None, the random number generator is the RandomState instance used by `np.random`.\n",
    "\n",
    "14. max_features : int, float, string or None, optional (default=None)\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `int(max_features * n_features)` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None, then `max_features=n_features`.\n",
    "\n",
    "    Choosing `max_features < n_features` leads to a reduction of variance\n",
    "    and an increase in bias.\n",
    "\n",
    "\n",
    "\n",
    "15. verbose : (default: 0) Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree.\n",
    "\n",
    "16. max_leaf_nodes : (default=None) Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "17. warm_start : (default: False) When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "18. presort : (default='deprecated') This parameter is deprecated and will be removed in v0.24.\n",
    "\n",
    " \n",
    "\n",
    "19. validation_fraction : (default 0.1) The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if ``n_iter_no_change`` is set to an integer.\n",
    "\n",
    "\n",
    "\n",
    "20. n_iter_no_change : (default: None) ``n_iter_no_change`` is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside ``validation_fraction`` size of the training data as validation and terminate training when validation score is not improving in all of the previous ``n_iter_no_change`` numbers of iterations. The split is stratified.\n",
    "\n",
    "\n",
    "\n",
    "21. tol : (default 1e-4) Tolerance for the early stopping. When the loss is not improving by at least tol for ``n_iter_no_change`` iterations (if set to a number), the training stops.\n",
    "\n",
    "\n",
    "22. ccp_alpha : (default=0.0) Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. \n",
    "\n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAGGING\n",
    "\n",
    "# Library\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model\n",
    "model = BaggingClassifier(base_estimator=None,      # SVC(), RandomForestClassifier(), GradientBoostingClassifier()\n",
    "                        n_estimators=50,\n",
    "                        max_samples=1.0,\n",
    "                        max_features=1.0,\n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False,\n",
    "                        oob_score=False,\n",
    "                        warm_start=False,\n",
    "                        n_jobs=None,\n",
    "                        random_state=None,\n",
    "                        verbose=0)\n",
    "\n",
    "\n",
    "model_BG = model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Accuracy\n",
    "pred_BG =model_BG.predict(X_test_scaled)\n",
    "acc_BG = accuracy_score(y_test, pred_BG)\n",
    "\n",
    "BG_Train = model_BG.score(X_train_scaled, y_train)\n",
    "\n",
    "pred_BG =model_BG.predict(X)\n",
    "pred_BG\n",
    "\n",
    "print(\"Training Accuracy:\",BG_Train)\n",
    "print(\"Testing Accuracy:\",acc_BG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\n",
    "#### BAGGING CLASSIFIER\n",
    "\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "\n",
    "1. base_estimator : (default=None) The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "\n",
    "2. n_estimators : (default=10) The number of base estimators in the ensemble.\n",
    "\n",
    "3. max_samples : (default=1.0) The number of samples to draw from X to train each base estimator.\n",
    "\n",
    "    - If int, then draw `max_samples` samples.\n",
    "    - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "\n",
    "4. max_features : (default=1.0) The number of features to draw from X to train each base estimator.\n",
    "\n",
    "    - If int, then draw `max_features` features.\n",
    "    - If float, then draw `max_features * X.shape[1]` features.\n",
    "\n",
    "5. bootstrap :  (default=True) Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n",
    "\n",
    "\n",
    "6. bootstrap_features : (default=False) Whether features are drawn with replacement.\n",
    "\n",
    "\n",
    "7. oob_score :  (default=False) Whether to use out-of-bag samples to estimate the generalization error.\n",
    "\n",
    "\n",
    "8. warm_start : (default=False) When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    " \n",
    "9. n_jobs : (default=None) The number of jobs to run in parallel for both :meth:`fit` and :meth:`predict`. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "10. random_state : (default=None)\n",
    "    - If int, random_state is the seed used by the random number generator;\n",
    "    - If RandomState instance, random_state is the random number generator;\n",
    "    - If None, the random number generator is the RandomState instance used by `np.random`.\n",
    "\n",
    "11. verbose : (default=0) Controls the verbosity when fitting and predicting.\n",
    "\n",
    "\n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "\n",
    "Comparison1 = pd.DataFrame({'Method':['Logistic Regression'], 'Train accuracy': LogR_Accuracy_Train, \n",
    "                           'Test accuracy': LogR_Accuracy_Test})\n",
    "Comparison1 = Comparison1[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "Comparison2 = pd.DataFrame({'Method':['Naive Bayes Regression'], 'Train accuracy': NB_Accuracy_Train, \n",
    "                           'Test accuracy': NB_Accuracy_Test},)\n",
    "Comparison2 = Comparison2[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "Comparison3 = pd.DataFrame({'Method':['KNN'], 'Train accuracy': KNN_Accuracy_Train, \n",
    "                           'Test accuracy': KNN_Accuracy_Test})\n",
    "Comparison3 = Comparison3[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "Comparison4 = pd.DataFrame({'Method':['SVM'], 'Train accuracy': DT_Train, \n",
    "                           'Test accuracy': DT_Test})\n",
    "Comparison4 = Comparison4[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "Comparison5 = pd.DataFrame({'Method':['Decision Tree'], 'Train accuracy': LogR_Accuracy_Train, \n",
    "                            'Test accuracy': LogR_Accuracy_Test})\n",
    "Comparison5 = Comparison5[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "\n",
    "Comparison6 = pd.DataFrame({'Method':['Random Forest'], 'Train accuracy': RF_Train, \n",
    "                           'Test accuracy': RF_Test})\n",
    "Comparison6 = Comparison6[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "\n",
    "Comparison7 = pd.DataFrame({'Method':['Bagging'], 'Train accuracy': BG_Train, \n",
    "                           'Test accuracy': acc_BG})\n",
    "Comparison7 = Comparison7[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "\n",
    "Comparison8 = pd.DataFrame({'Method':['Gradient Boosting'], 'Train accuracy': GB_Train, \n",
    "                           'Test accuracy': acc_GB})\n",
    "Comparison8 = Comparison8[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "\n",
    "\n",
    "Comparison9 = pd.DataFrame({'Method':['ADA Boosting'], 'Train accuracy': AB_Train, \n",
    "                           'Test accuracy': acc_AB})\n",
    "Comparison9 = Comparison9[['Method', 'Train accuracy', 'Test accuracy']]\n",
    "\n",
    "Comparison = pd.concat([Comparison1, Comparison2,Comparison3,Comparison4,Comparison5,Comparison6,\n",
    "                        Comparison7,Comparison8,Comparison9])\n",
    "\n",
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle    # Job lib , json\n",
    "Pickled_model = pickle.dumps(model_BG) \n",
    "print(Fore.GREEN+\"\\nCompleted pickling the model\")\n",
    "\n",
    "print(Fore.GREEN+\"\\nReload the pickled model\")\n",
    "Pickled_Load = pickle.loads(Pickled_model)\n",
    "\n",
    "y_pred_1 = Pickled_Load.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. GUI development:\n",
    "\n",
    "- Design a clickable GUI desk application or web service application.\n",
    "- This GUI should allow the user to input all future values and on a click use these values on the trained model above to predict.\n",
    "- It should display the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# App window\n",
    "win = tk.Tk()\n",
    "win.title('GUI')  # Window Title\n",
    "\n",
    "# Column 1: Gender -------------------------------------------------------------------------------------------\n",
    "\n",
    "Gender=ttk.Label(win,text=\"Gender\")\n",
    "Gender.grid(row=0,column=0,sticky=tk.W)\n",
    "\n",
    "Gender_var=tk.StringVar()\n",
    "Gender_combobox=ttk.Combobox(win,width=13,textvariable=Gender_var)\n",
    "Gender_combobox['values']=('Male','Female')\n",
    "Gender_combobox.grid(row=0,column=1)\n",
    "\n",
    "\n",
    "# Column 2: Senior Citizen -------------------------------------------------------------------------------------\n",
    "\n",
    "SrCitizen=ttk.Label(win,text=\"Senior Citizen\")\n",
    "SrCitizen.grid(row=1,column=0,sticky=tk.W)\n",
    "\n",
    "SrCitizen_var=tk.StringVar()\n",
    "SrCitizen_combobox=ttk.Combobox(win,width=13,textvariable=SrCitizen_var)\n",
    "SrCitizen_combobox['values']=('1','0')\n",
    "SrCitizen_combobox.grid(row=1,column=1)\n",
    "\n",
    "\n",
    "# Column 3: Partner -------------------------------------------------------------------------------------\n",
    "\n",
    "Partner=ttk.Label(win,text=\"Partner\")\n",
    "Partner.grid(row=2,column=0,sticky=tk.W)\n",
    "\n",
    "Partner_var=tk.StringVar()\n",
    "Partner_combobox=ttk.Combobox(win,width=13,textvariable=Partner_var)\n",
    "Partner_combobox['values']=('Yes','No')\n",
    "Partner_combobox.grid(row=2,column=1)\n",
    "\n",
    "\n",
    "# Column 4: Dependents -------------------------------------------------------------------------------------\n",
    "Dependents=ttk.Label(win,text=\"Dependents\")\n",
    "Dependents.grid(row=3,column=0,sticky=tk.W)\n",
    "\n",
    "Dependents_var=tk.StringVar()\n",
    "Dependents_combobox=ttk.Combobox(win,width=13,textvariable=Dependents_var)\n",
    "Dependents_combobox['values']=('Yes','No')\n",
    "Dependents_combobox.grid(row=3,column=1)\n",
    "\n",
    "# Column 5: Tenure -----------------------------------------------------------------\n",
    "Tenure=ttk.Label(win,text=\"Tenure\")\n",
    "Tenure.grid(row=4,column=0,sticky=tk.W)\n",
    "\n",
    "Tenure_var=tk.StringVar()\n",
    "Tenure_entrybox=ttk.Entry(win,width=16,textvariable=Tenure_var)\n",
    "Tenure_entrybox.grid(row=4,column=1)\n",
    "\n",
    "\n",
    "# Column 6: Phone Services -----------------------------------------------------------------\n",
    "PhoneServices=ttk.Label(win,text=\"Phone Services\")\n",
    "PhoneServices.grid(row=5,column=0,sticky=tk.W)\n",
    "\n",
    "PhoneServices_var=tk.StringVar()\n",
    "PhoneServices_combobox=ttk.Combobox(win,width=13,textvariable=PhoneServices_var)\n",
    "PhoneServices_combobox['values']=('Yes','No')\n",
    "PhoneServices_combobox.grid(row=5,column=1)\n",
    "\n",
    "\n",
    "\n",
    "# Column 7: Multiple Lines -----------------------------------------------------------------\n",
    "MultipleLines=ttk.Label(win,text=\"Multiple Lines\")\n",
    "MultipleLines.grid(row=6,column=0,sticky=tk.W)\n",
    "\n",
    "MultipleLines_var=tk.StringVar()\n",
    "MultipleLines_combobox=ttk.Combobox(win,width=13,textvariable=MultipleLines_var)\n",
    "MultipleLines_combobox['values']=('Yes','No','No phone service')\n",
    "MultipleLines_combobox.grid(row=6,column=1)\n",
    "\n",
    "\n",
    "# Column 8: Internet Service -----------------------------------------------------------------\n",
    "InternetService=ttk.Label(win,text=\"Internet Service\")\n",
    "InternetService.grid(row=7,column=0,sticky=tk.W)\n",
    "\n",
    "InternetService_var=tk.StringVar()\n",
    "InternetService_combobox=ttk.Combobox(win,width=13,textvariable=InternetService_var)\n",
    "InternetService_combobox['values']=('DSL','Fibre Optic','No')\n",
    "InternetService_combobox.grid(row=7,column=1)\n",
    "\n",
    "\n",
    "# Column 9: Online Security -----------------------------------------------------------------\n",
    "OnlineSecurity=ttk.Label(win,text=\"Online Security\")\n",
    "OnlineSecurity.grid(row=8,column=0,sticky=tk.W)\n",
    "\n",
    "OnlineSecurity_var=tk.StringVar()\n",
    "OnlineSecurity_combobox=ttk.Combobox(win,width=13,textvariable=OnlineSecurity_var)\n",
    "OnlineSecurity_combobox['values']=('Yes','No','No Internet service')\n",
    "OnlineSecurity_combobox.grid(row=8,column=1)\n",
    "\n",
    "\n",
    "# Column 10: Online Backup -----------------------------------------------------------------\n",
    "OnlineBackup=ttk.Label(win,text=\"Online Backup\")\n",
    "OnlineBackup.grid(row=9,column=0,sticky=tk.W)\n",
    "\n",
    "OnlineBackup_var=tk.StringVar()\n",
    "OnlineBackup_combobox=ttk.Combobox(win,width=13,textvariable=OnlineBackup_var)\n",
    "OnlineBackup_combobox['values']=('Yes','No','No Internet service')\n",
    "OnlineBackup_combobox.grid(row=9,column=1)\n",
    "\n",
    "\n",
    "# Column 11: Device Protection -----------------------------------------------------------------\n",
    "DeviceProtection=ttk.Label(win,text=\"Device Protection\")\n",
    "DeviceProtection.grid(row=10,column=0,sticky=tk.W)\n",
    "\n",
    "DeviceProtection_var=tk.StringVar()\n",
    "DeviceProtection_combobox=ttk.Combobox(win,width=13,textvariable=DeviceProtection_var)\n",
    "DeviceProtection_combobox['values']=('Yes','No','No Internet service')\n",
    "DeviceProtection_combobox.grid(row=10,column=1)\n",
    "\n",
    "\n",
    "# Column 12: Tech Support -----------------------------------------------------------------\n",
    "TechSupport=ttk.Label(win,text=\"Tech Support\")\n",
    "TechSupport.grid(row=11,column=0,sticky=tk.W)\n",
    "\n",
    "TechSupport_var=tk.StringVar()\n",
    "TechSupport_combobox=ttk.Combobox(win,width=13,textvariable=TechSupport_var)\n",
    "TechSupport_combobox['values']=('Yes','No','No Internet service')\n",
    "TechSupport_combobox.grid(row=11,column=1)\n",
    "\n",
    "\n",
    "# Column 13: Streaming TV -----------------------------------------------------------------\n",
    "StreamingTV=ttk.Label(win,text=\"Streaming TV\")\n",
    "StreamingTV.grid(row=12,column=0,sticky=tk.W)\n",
    "\n",
    "StreamingTV_var=tk.StringVar()\n",
    "StreamingTV_combobox=ttk.Combobox(win,width=13,textvariable=StreamingTV_var)\n",
    "StreamingTV_combobox['values']=('Yes','No','No Internet service')\n",
    "StreamingTV_combobox.grid(row=12,column=1)\n",
    "\n",
    "\n",
    "# Column 14: Streaming Movies\n",
    "StreamingMovies=ttk.Label(win,text=\"Streaming Movies\")\n",
    "StreamingMovies.grid(row=13,column=0,sticky=tk.W)\n",
    "\n",
    "StreamingMovies_var=tk.StringVar()\n",
    "StreamingMovies_combobox=ttk.Combobox(win,width=13,textvariable=StreamingMovies_var)\n",
    "StreamingMovies_combobox['values']=('Yes','No','No Internet service')\n",
    "StreamingMovies_combobox.grid(row=13,column=1)\n",
    "\n",
    "\n",
    "# Column 15: Contract -----------------------------------------------------------------\n",
    "Contract=ttk.Label(win,text=\"Contract\")\n",
    "Contract.grid(row=14,column=0,sticky=tk.W)\n",
    "\n",
    "Contract_var=tk.StringVar()\n",
    "Contract_combobox=ttk.Combobox(win,width=13,textvariable=Contract_var)\n",
    "Contract_combobox['values']=('Month-to-month','One year','Two year')\n",
    "Contract_combobox.grid(row=14,column=1)\n",
    "\n",
    "\n",
    "# Column 16: Paperless Billing -----------------------------------------------------------------\n",
    "PaperlessBilling=ttk.Label(win,text=\"Paperless Billing\")\n",
    "PaperlessBilling.grid(row=15,column=0,sticky=tk.W)\n",
    "\n",
    "PaperlessBilling_var=tk.StringVar()\n",
    "PaperlessBilling_combobox=ttk.Combobox(win,width=13,textvariable=PaperlessBilling_var)\n",
    "PaperlessBilling_combobox['values']=('Yes','No')\n",
    "PaperlessBilling_combobox.grid(row=15,column=1)\n",
    "\n",
    "\n",
    "# Column 17: Payment Method ----------------------------------------------------------------\n",
    "PaymentMethod=ttk.Label(win,text=\"Payment Method\")\n",
    "PaymentMethod.grid(row=16,column=0,sticky=tk.W)\n",
    "\n",
    "PaymentMethod_var=tk.StringVar()\n",
    "PaymentMethod_combobox=ttk.Combobox(win,width=13,textvariable=PaymentMethod_var)\n",
    "PaymentMethod_combobox['values']=('Bank transfer','Credit card','Electronic check','Mailed check')\n",
    "PaymentMethod_combobox.grid(row=16,column=1)\n",
    "\n",
    "\n",
    "# Column 18: Monthly Charges ----------------------------------------------------------------\n",
    "MonthlyCharges=ttk.Label(win,text=\"Monthly Charges\")\n",
    "MonthlyCharges.grid(row=17,column=0,sticky=tk.W)\n",
    "\n",
    "MonthlyCharges_var=tk.StringVar()\n",
    "MonthlyCharges_entrybox=ttk.Entry(win,width=16,textvariable=MonthlyCharges_var)\n",
    "MonthlyCharges_entrybox.grid(row=17,column=1)\n",
    "\n",
    "\n",
    "# Column 19: Total Charges\n",
    "TotalCharges=ttk.Label(win,text=\"Total Charges\")\n",
    "TotalCharges.grid(row=18,column=0,sticky=tk.W)\n",
    "\n",
    "TotalCharges_var=tk.StringVar()\n",
    "TotalCharges_entrybox=ttk.Entry(win,width=16,textvariable=TotalCharges_var)\n",
    "TotalCharges_entrybox.grid(row=18,column=1)\n",
    "\n",
    "import pandas as pd\n",
    "DF = pd.DataFrame()\n",
    "    \n",
    "\n",
    "\n",
    "# Create predict button -----------------------------------------------------------------\n",
    "\n",
    "def action():\n",
    "    global DB\n",
    "    import pandas as pd\n",
    "    DF = pd.DataFrame(columns=['Gender','Senior Citizen','Partner','Dependents','Tenure','Phone Services',\n",
    "                               'Multiple Lines','Internet Services','Online Security','Online Backup',\n",
    "                               'Device Protection','Tech Support','Streaming TV','Streaming Movies','Contract',\n",
    "                              'Paperless Billing', 'Payment Method','Monthly Charges','Total Charges'])\n",
    "    \n",
    "    # Column 1: Gender\n",
    "    GENDER=Gender_var.get()\n",
    "    if GENDER=='Female':\n",
    "        DF.loc[0,'Gender']=0\n",
    "    elif GENDER=='Male':\n",
    "        DF.loc[0,'Gender']=1\n",
    "  \n",
    "\n",
    "    # Column 2: Senior Citizen \n",
    "    SC=SrCitizen_var.get()\n",
    "    DF.loc[0,'Senior Citizen']=SC\n",
    "\n",
    "    # Column 3: Partner\n",
    "    PARTNER=Partner_var.get()\n",
    "    if PARTNER=='No':\n",
    "        DF.loc[0,'Partner']=0\n",
    "    elif PARTNER=='Yes':\n",
    "        DF.loc[0,'Partner']=1\n",
    "    \n",
    "    # Column 4: Dependents\n",
    "    DEPENDENT=Dependents_var.get()\n",
    "    if DEPENDENT=='No':\n",
    "        DF.loc[0,'Dependents']=0\n",
    "    elif DEPENDENT=='Yes':\n",
    "        DF.loc[0,'Dependents']=1\n",
    "    \n",
    "    # Column 5: Tenure\n",
    "    TENURE=Tenure_var.get()\n",
    "    DF.loc[0,'Tenure']=TENURE\n",
    "    \n",
    "    # Column 6: Phone Services\n",
    "    PHONESERVICES=PhoneServices_var.get()\n",
    "    if PHONESERVICES=='No':\n",
    "        DF.loc[0,'Phone Services']=0\n",
    "    elif PHONESERVICES=='Yes':\n",
    "        DF.loc[0,'Phone Services']=1\n",
    "    \n",
    "    # Column 7: Multiple Lines \n",
    "    MULTIPLELINES=MultipleLines_var.get()\n",
    "    if MULTIPLELINES=='No':\n",
    "        DF.loc[0,'Multiple Lines']=0\n",
    "    elif MULTIPLELINES=='No phone service':\n",
    "        DF.loc[0,'Multiple Lines']=1\n",
    "    elif MULTIPLELINES=='Yes':\n",
    "        DF.loc[0,'Multiple Lines']=2\n",
    "    \n",
    "    # Column 8: Internet Services \n",
    "    INTERNETSERVICES=InternetService_var.get()\n",
    "    if INTERNETSERVICES=='DSL':\n",
    "        DF.loc[0,'Internet Services']=0\n",
    "    elif INTERNETSERVICES=='Fibre Optic':\n",
    "        DF.loc[0,'Internet Services']=1\n",
    "    elif INTERNETSERVICES=='No':\n",
    "        DF.loc[0,'Internet Services']=2\n",
    "    \n",
    "    \n",
    "    # Column 9: Online Security \n",
    "    ONLINESECURITY=OnlineSecurity_var.get()\n",
    "    if ONLINESECURITY=='No':\n",
    "        DF.loc[0,'Online Security']=0\n",
    "    elif ONLINESECURITY=='No Internet service':\n",
    "        DF.loc[0,'Online Security']=1\n",
    "    elif ONLINESECURITY=='Yes':\n",
    "        DF.loc[0,'Online Security']=2\n",
    "        \n",
    "        \n",
    "    # Column 10: Online Backup \n",
    "    ONLINEBACKUP=OnlineBackup_var.get()\n",
    "    if ONLINEBACKUP=='No':\n",
    "        DF.loc[0,'Online Backup']=0\n",
    "    elif ONLINEBACKUP=='No Internet service':\n",
    "        DF.loc[0,'Online Backup']=1\n",
    "    elif ONLINEBACKUP=='Yes':\n",
    "        DF.loc[0,'Online Backup']=2\n",
    "    \n",
    "    \n",
    "    # Column 11: Device Protection \n",
    "    DEVICEPROTECTION=DeviceProtection_var.get()\n",
    "    if DEVICEPROTECTION=='No':\n",
    "        DF.loc[0,'Device Protection']=0\n",
    "    elif DEVICEPROTECTION=='No Internet service':\n",
    "        DF.loc[0,'Device Protection']=1\n",
    "    elif DEVICEPROTECTION=='Yes':\n",
    "        DF.loc[0,'Device Protection']=2\n",
    "    \n",
    "    # Column 12: Tech Support\n",
    "    TECHSUPPORT=TechSupport_var.get()\n",
    "    if TECHSUPPORT=='No':\n",
    "        DF.loc[0,'Tech Support']=0\n",
    "    elif TECHSUPPORT=='No Internet service':\n",
    "        DF.loc[0,'Tech Support']=1\n",
    "    elif TECHSUPPORT=='Yes':\n",
    "        DF.loc[0,'Tech Support']=2\n",
    "    \n",
    "    # Column 13: Streaming TV\n",
    "    STREAMINGTV=StreamingTV_var.get()\n",
    "    if STREAMINGTV=='No':\n",
    "        DF.loc[0,'Streaming TV']=0\n",
    "    elif STREAMINGTV=='No Internet service':\n",
    "        DF.loc[0,'Streaming TV']=1\n",
    "    elif STREAMINGTV=='Yes':\n",
    "        DF.loc[0,'Streaming TV']=2\n",
    "    \n",
    "    \n",
    "    # Column 14: Streaming Movies\n",
    "    STREAMINGMOVIES=StreamingMovies_var.get()\n",
    "    if STREAMINGMOVIES=='No':\n",
    "        DF.loc[0,'Streaming Movies']=0\n",
    "    elif STREAMINGMOVIES=='No Internet service':\n",
    "        DF.loc[0,'TStreaming Movies']=1\n",
    "    elif STREAMINGMOVIES=='Yes':\n",
    "        DF.loc[0,'Streaming Movies']=2\n",
    "    \n",
    "    \n",
    "    # Column 15: Contract\n",
    "    CONTRACT=Contract_var.get()\n",
    "    if CONTRACT=='Month-to-month':\n",
    "        DF.loc[0,'Contract']=0\n",
    "    elif CONTRACT=='One year':\n",
    "        DF.loc[0,'Contract']=1\n",
    "    elif CONTRACT=='Two year':\n",
    "        DF.loc[0,'Contract']=2\n",
    "    \n",
    "    \n",
    "    # Column 16: Paperless Billing\n",
    "    PAPERLESSBILLING=PaperlessBilling_var.get()\n",
    "    if PAPERLESSBILLING=='No':\n",
    "        DF.loc[0,'Paperless Billing']=0\n",
    "    elif PAPERLESSBILLING=='Yes':\n",
    "        DF.loc[0,'Paperless Billing']=1\n",
    "    \n",
    "    \n",
    "    # Column 17: Payment Method\n",
    "    PAYMENTMETHOD=PaymentMethod_var.get()\n",
    "    if PAYMENTMETHOD=='Bank transfer':\n",
    "        DF.loc[0,'Payment Method']=0\n",
    "    elif PAYMENTMETHOD=='Credit card':\n",
    "        DF.loc[0,'Payment Method']=1\n",
    "    elif PAYMENTMETHOD=='Electronic check':\n",
    "        DF.loc[0,'Payment Method']=2\n",
    "    elif PAYMENTMETHOD=='Mailed check':\n",
    "        DF.loc[0,'Payment Method']=3\n",
    "    \n",
    "    \n",
    "    # Column 18: Monthly Charges\n",
    "    MONTHLYCHARGES=MonthlyCharges_var.get()\n",
    "    DF.loc[0,'Monthly Charges']=MONTHLYCHARGES\n",
    "    \n",
    "    # Column 19: Total Charges\n",
    "    TOTALCHARGES=TotalCharges_var.get()\n",
    "    DF.loc[0,'Total Charges']=TOTALCHARGES\n",
    "    \n",
    "    \n",
    "    print(DF.shape)\n",
    "    DB=DF\n",
    "    \n",
    "\n",
    "Predict_button=ttk.Button(win,text=\"Submit\",command=action)\n",
    "Predict_button.grid(row=19,column=0)\n",
    "\n",
    "\n",
    "\n",
    "def Output():\n",
    "    DB[\"Gender\"] = pd.to_numeric(DB[\"Gender\"])\n",
    "    DB[\"Senior Citizen\"] = pd.to_numeric(DB[\"Senior Citizen\"])\n",
    "    DB[\"Partner\"] = pd.to_numeric(DB[\"Partner\"])\n",
    "    DB[\"Dependents\"] = pd.to_numeric(DB[\"Dependents\"])\n",
    "    DB[\"Tenure\"] = pd.to_numeric(DB[\"Tenure\"])\n",
    "    DB[\"Phone Services\"] = pd.to_numeric(DB[\"Phone Services\"])\n",
    "    DB[\"Multiple Lines\"] = pd.to_numeric(DB[\"Multiple Lines\"])\n",
    "    DB[\"Internet Services\"] = pd.to_numeric(DB[\"Internet Services\"])\n",
    "    DB[\"Online Security\"] = pd.to_numeric(DB[\"Online Security\"])\n",
    "    DB[\"Online Backup\"] = pd.to_numeric(DB[\"Online Backup\"])\n",
    "    DB[\"Device Protection\"] = pd.to_numeric(DB[\"Device Protection\"])\n",
    "    DB[\"Online Security\"] = pd.to_numeric(DB[\"Online Security\"])\n",
    "    DB[\"Streaming TV\"] = pd.to_numeric(DB[\"Streaming TV\"])\n",
    "    DB[\"Streaming Movies\"] = pd.to_numeric(DB[\"Streaming Movies\"])\n",
    "    DB[\"Contract\"] = pd.to_numeric(DB[\"Contract\"])\n",
    "    DB[\"Paperless Billing\"] = pd.to_numeric(DB[\"Paperless Billing\"])\n",
    "    DB[\"Payment Method\"] = pd.to_numeric(DB[\"Payment Method\"])\n",
    "    DB[\"Monthly Charges\"] = pd.to_numeric(DB[\"Monthly Charges\"])\n",
    "    DB[\"Total Charges\"] = pd.to_numeric(DB[\"Total Charges\"])\n",
    "    \n",
    "    output=model_AB.predict(DB)\n",
    "    if output==1:\n",
    "        result='Yes'\n",
    "    elif output==0:\n",
    "        result='No'\n",
    "    # Blank empty window to print prediction\n",
    "    Predict_entrybox=ttk.Entry(win,width=16)\n",
    "    Predict_entrybox.grid(row=20,column=1)\n",
    "    Predict_entrybox.insert(1,str(result))\n",
    "\n",
    "\n",
    "Predict_button=ttk.Button(win,text=\"Predict\",command=Output)\n",
    "Predict_button.grid(row=20,column=0)\n",
    "\n",
    "\n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conclusion and improvisation: \n",
    "\n",
    "- Write your conclusion on the results. \n",
    "    - A relevant brief of all the processes above.\n",
    "    \n",
    "- Detailed suggestions or improvements or on quality, quantity, variety, velocity, veracity etc. on the data points collected by the bank to perform a better data analysis in future. \n",
    "\n",
    "    - This is a very open topic which might vary for each learner. Award scores accordingly as per relevant and logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Â©Great Learning. Proprietary content. All Rights Reserved. Unauthorised use or distribution prohibited"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
